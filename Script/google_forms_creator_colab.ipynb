{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 📝 Google Forms Creator for SME Evaluation\n",
        "\n",
        "**Simple Google Colab notebook to create forms for expert evaluation of AI models**\n",
        "\n",
        "This notebook creates Google Forms based on your sample data for Subject Matter Expert (SME) evaluation.\n",
        "\n",
        "## 🚀 Quick Start:\n",
        "1. Upload your JSON data file to Colab\n",
        "2. Update the `PROJECT_ID` and `JSON_FILE_PATH` below\n",
        "3. Run all cells\n",
        "4. Get your form URLs!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📦 Setup and Authentication\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -qqq google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from typing import List, Dict, Any, Optional\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from google.auth import default\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔧 CONFIGURATION - UPDATE THESE VALUES\n",
        "PROJECT_ID = \"black-heuristic-xxx-f6\"  # Your Google Cloud Project ID\n",
        "BATCH_ID = 1\n",
        "JSON_FILE_PATH = f\"/content/sample_data_template_{BATCH_ID}.json\"\n",
        "\n",
        "FORM_TITLE = f\"BEACON LLM Model Evaluation for the Severity Assessment\" \n",
        "\n",
        "print(f\"🔧 Project ID: {PROJECT_ID}\")\n",
        "print(f\"📂 Data file: {JSON_FILE_PATH}\")\n",
        "print(f\"📝 Form title: {FORM_TITLE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Set environment variables\n",
        "os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID\n",
        "os.environ['GCLOUD_PROJECT'] = PROJECT_ID\n",
        "\n",
        "# Authenticate\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "\n",
        "if hasattr(creds, 'with_quota_project'):\n",
        "    creds = creds.with_quota_project(PROJECT_ID)\n",
        "\n",
        "# Build Forms service\n",
        "forms_service = build('forms', 'v1', credentials=creds)\n",
        "print(\"Google Forms API connected successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📂 Load your JSON data\n",
        "print(f\"📂 Loading data from: {JSON_FILE_PATH}\")\n",
        "\n",
        "try:\n",
        "    with open(JSON_FILE_PATH, 'r', encoding='utf-8') as f:\n",
        "        sample_data = json.load(f)\n",
        "    print(f\"✅ Loaded {len(sample_data)} samples!\")\n",
        "    \n",
        "    # Show first sample structure\n",
        "    if sample_data:\n",
        "        print(\"\\n📋 Sample data structure:\")\n",
        "        first_sample = sample_data[0]\n",
        "        for key in first_sample.keys():\n",
        "            value = str(first_sample[key])[:100] + \"...\" if len(str(first_sample[key])) > 100 else first_sample[key]\n",
        "            print(f\"  • {key}: {value}\")\n",
        "            \n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ File not found: {JSON_FILE_PATH}\")\n",
        "    \n",
        "print(f\"\\n🎯 Ready to create form with {len(sample_data)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 Create the Google Form\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add instructions\n",
        "\n",
        "# Improved format optimized for Google Forms plain text display\n",
        "instructions = \"\"\"\n",
        "OVERVIEW\n",
        "You are participating in a research study to evaluate AI-generated outbreak severity assessments for news articles about health threats.\n",
        "\n",
        "\n",
        "\n",
        "WHAT YOU'LL REVIEW\n",
        "\n",
        "Article: News article from HealthMap's archived reports of historical disease outbreaks\n",
        "\n",
        "Reference Assessment: Current \"gold standard\" baseline created by GPT-o1 (advanced AI model)\n",
        "• Includes severity score and detailed reasoning\n",
        "• Used as our training reference point\n",
        "\n",
        "Model A & Model B Assessments: Two randomized AI model outputs for comparison\n",
        "• One fine-tuned model (BEACON LLM)\n",
        "• One base model (Llama-3.1-8B)\n",
        "• Order is randomized to prevent bias\n",
        "\n",
        "\n",
        "KEY DEFINITIONS\n",
        "\n",
        "Severity Score Scale (1-5):\n",
        "1 = Very Low Risk \n",
        "2 = Low Risk \n",
        "3 = Moderate Risk\n",
        "4 = High Risk\n",
        "5 = Very High Risk\n",
        "\n",
        "Severity Reasoning:\n",
        "Detailed justification explaining the assigned score based on relevant epidemiological, clinical, and contextual factors for the specific risk domain.\n",
        "\n",
        "\n",
        "YOUR EVALUATION TASKS\n",
        "\n",
        "Task 1: Reference Validation\n",
        "Purpose: Validate our \"gold standard\" baseline assessment\n",
        "Your Role: Evaluate whether GPT-o1's risk assessment and reasoning are accurate and appropriate\n",
        "Importance: Ensures our training data quality\n",
        "\n",
        "Task 2: Model Comparison\n",
        "Purpose: Compare two different AI approaches\n",
        "Your Role: Determine which model provides better risk assessment and reasoning\n",
        "Focus: Overall quality, not just agreement with reference\n",
        "\n",
        "Task 3: Expert Assessment (Optional)\n",
        "Purpose: Provide independent expert judgment\n",
        "When to Use: When you believe all AI models significantly over- or under-estimate \n",
        "Your Role: Provide your own expert severity score with justification\n",
        "\n",
        "\n",
        "EVALUATION CRITERIA\n",
        "\n",
        "Please evaluate based on:\n",
        "• Scientific Accuracy: Correctness of epidemiological and medical information\n",
        "• Risk Appropriateness: How well the severity score matches the described threat level\n",
        "• Reasoning Quality: Completeness and quality of the justification\n",
        "\n",
        "\n",
        "SUPPORT\n",
        "For technical issues or questions about this evaluation, please contact: jmyang@bu.edu\n",
        "\n",
        "Thank you for contributing your expertise to improve BEACON LLMS.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Create basic form\n",
        "form_request = {\"info\": {\"title\": f\"{FORM_TITLE} - Batch {BATCH_ID}\"}}\n",
        "\n",
        "form = forms_service.forms().create(body=form_request).execute()\n",
        "form_id = form['formId']\n",
        "\n",
        "# Step 2: Add description\n",
        "try:\n",
        "    forms_service.forms().batchUpdate(\n",
        "        formId=form_id,\n",
        "        body={\n",
        "            \"requests\": [{\n",
        "                \"updateFormInfo\": {\n",
        "                    \"info\": {\n",
        "                        \"title\": f\"{FORM_TITLE} - Batch {BATCH_ID}\",\n",
        "                        \"description\": f\"{instructions}\"\n",
        "                    },\n",
        "                    \"updateMask\": \"description\"\n",
        "                }\n",
        "            }]\n",
        "        }\n",
        "    ).execute()\n",
        "except Exception as e:\n",
        "    print(f\"Description error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📝 Add Form Content\n",
        "\n",
        "all_requests = []\n",
        "current_index = 0\n",
        "\n",
        "# Add SME Email field\n",
        "all_requests.append({\n",
        "    \"createItem\": {\n",
        "        \"item\": {\n",
        "            \"title\": \"Your Email (Required)\",\n",
        "            \"description\": \"Please enter your email address\",\n",
        "            \"questionItem\": {\n",
        "                \"question\": {\n",
        "                    \"required\": True,\n",
        "                    \"textQuestion\": {\"paragraph\": False}\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"location\": {\"index\": current_index}\n",
        "    }\n",
        "})\n",
        "current_index += 1\n",
        "\n",
        "# Add questions for each sample - with page breaks and separate article questions\n",
        "for i, sample in enumerate(sample_data):\n",
        "    sample_num = i + 1\n",
        "    \n",
        "    ### Add page break before each new sample (except the first)\n",
        "    if i > 0:\n",
        "        all_requests.append({\n",
        "            \"createItem\": {\n",
        "                \"item\": {\n",
        "                    \"pageBreakItem\": {}\n",
        "                },\n",
        "                \"location\": {\"index\": current_index}\n",
        "            }\n",
        "        })\n",
        "        current_index += 1\n",
        "    \n",
        "    ### Add the sample header with domain and factors only\n",
        "    all_requests.append({\n",
        "        \"createItem\": {\n",
        "            \"item\": {\n",
        "                \"title\": f\"Sample {sample_num}\",\n",
        "                \"description\": f\"Domain: {sample.get('domain', 'Unknown Domain')}\\n\\nFactors: {sample.get('factors', 'Unknown Factors')}\",\n",
        "                \"textItem\": {}\n",
        "            },\n",
        "            \"location\": {\"index\": current_index}\n",
        "        }\n",
        "    })\n",
        "    current_index += 1\n",
        "    \n",
        "    ### Add article content as a separate question on the same page\n",
        "    all_requests.append({\n",
        "        \"createItem\": {\n",
        "            \"item\": {\n",
        "                \"title\": f\"Article Content - Sample {sample_num}\",\n",
        "                \"description\": sample.get('article_content', 'Article content'),\n",
        "                \"textItem\": {\n",
        "                    \"paragraph\": True\n",
        "                }\n",
        "            },\n",
        "            \"location\": {\"index\": current_index}\n",
        "        }\n",
        "    })\n",
        "    current_index += 1\n",
        "    # Add Reference assessment\n",
        "    \n",
        "    all_requests.append({\n",
        "        \"createItem\": {\n",
        "            \"item\": {\n",
        "                \"title\": f\"Task 1. Reference Assessment (Generated by OpenAI GPT-O1) - Sample {sample_num}\",\n",
        "                \"description\": f\"\"\"\n",
        "Severity Score: {sample.get('reference_score', 'X')}/5\n",
        "Severity Reasoning: {sample.get('reference_reasoning', 'Reference reasoning')}\"\"\",\n",
        "                \"textItem\": {}\n",
        "            },\n",
        "            \"location\": {\"index\": current_index}\n",
        "        }\n",
        "    })\n",
        "    current_index += 1\n",
        "\n",
        "    # Reference risk score question\n",
        "    all_requests.append({\n",
        "        \"createItem\": {\n",
        "            \"item\": {\n",
        "                \"title\": f\"Q1. Is the reference assessment's risk score appropriate for this scenario? - Sample {sample_num} (Required)\",\n",
        "                \"description\": f\"Severity Score: {sample.get('reference_score', 'X')}/5\",\n",
        "                \"questionItem\": {\n",
        "                    \"question\": {\n",
        "                        \"required\": True,\n",
        "                        \"choiceQuestion\": {\n",
        "                            \"type\": \"RADIO\",\n",
        "                            \"options\": [\n",
        "                                {\"value\": \"Too low (should be higher)\"},\n",
        "                                {\"value\": \"Appropriate (correct level)\"},\n",
        "                                {\"value\": \"Too high (should be lower)\"}\n",
        "                            ]\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            \"location\": {\"index\": current_index}\n",
        "        }\n",
        "    })\n",
        "    current_index += 1\n",
        "    \n",
        "    # Reference reasoinng question\n",
        "    all_requests.append({\n",
        "        \"createItem\": {\n",
        "            \"item\": {\n",
        "                \"title\": f\"Q2. Rate the accuracy and quality of the Reference reasoning - Sample {sample_num} (Required)\",\n",
        "                \"description\": f\"Severity Reasoning: {sample.get('reference_reasoning', 'Reference reasoning')}\",\n",
        "                \"questionItem\": {\n",
        "                    \"question\": {\n",
        "                        \"required\": True,\n",
        "                        \"choiceQuestion\": {\n",
        "                            \"type\": \"RADIO\",\n",
        "                            \"options\": [\n",
        "                                {\"value\": \"Poor (Major errors)\"},\n",
        "                                {\"value\": \"Average (Acceptable)\"},\n",
        "                                {\"value\": \"Good (Accurate)\"}\n",
        "                            ]\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            \"location\": {\"index\": current_index}\n",
        "        }\n",
        "    })\n",
        "    current_index += 1\n",
        "\n",
        "\n",
        "        \n",
        "    # Model comparison display\n",
        "    \n",
        "    all_requests.append({\n",
        "        \"createItem\": {\n",
        "            \"item\": {\n",
        "                \"title\": f\"Task 2. Model Comparison - Sample {sample_num}\",\n",
        "                \"description\": f\"\"\"\n",
        "Model A Severity Score: {sample.get('model_a_score', 'Y')}/5\\n\\n\n",
        "Model A Severity Reasoning: {sample.get('model_a_reasoning', 'Model A reasoning')}\\n\\n\n",
        "Model B Severity Score: {sample.get('model_b_score', 'Z')}/5  \\n\\n\n",
        "Model B Severity Reasoning: {sample.get('model_b_reasoning', 'Model B reasoning')}\"\"\",\n",
        "                \"textItem\": {}\n",
        "            },\n",
        "            \"location\": {\"index\": current_index}\n",
        "        }\n",
        "    })\n",
        "    current_index += 1\n",
        "    \n",
        "    # Model comparison question\n",
        "    all_requests.append({\n",
        "        \"createItem\": {\n",
        "            \"item\": {\n",
        "                \"title\": f\"Q3. Which model severity score assessment is more accurate? - Sample {sample_num} (Required)\",\n",
        "                \"description\": f\"Model A Severity Score: {sample.get('model_a_score', 'Y')}/5\\n\\nModel B Severity Score: {sample.get('model_b_score', 'Z')}/5  \\n\\n\",\n",
        "                \"questionItem\": {\n",
        "                    \"question\": {\n",
        "                        \"required\": True,\n",
        "                        \"choiceQuestion\": {\n",
        "                            \"type\": \"RADIO\",\n",
        "                            \"options\": [\n",
        "                                {\"value\": \"Model A is clearly better\"},\n",
        "                                {\"value\": \"Model A is slightly better\"},\n",
        "                                {\"value\": \"Both are equivalent\"},\n",
        "                                {\"value\": \"Model B is slightly better\"},\n",
        "                                {\"value\": \"Model B is clearly better\"}\n",
        "                            ]\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            \"location\": {\"index\": current_index}\n",
        "        }\n",
        "    })\n",
        "    current_index += 1\n",
        "\n",
        "    # Model comparison question\n",
        "    all_requests.append({\n",
        "        \"createItem\": {\n",
        "            \"item\": {\n",
        "                \"title\": f\"Q4. Which model provides BETTER REASONING and justification? - Sample {sample_num} (Required)\",\n",
        "                \"description\": f\"Model A Severity Reasoning: {sample.get('model_a_reasoning', 'Model A reasoning')}\\n\\nModel B Severity Reasoning: {sample.get('model_b_reasoning', 'Model B reasoning')}\",\n",
        "                \"questionItem\": {\n",
        "                    \"question\": {\n",
        "                        \"required\": True,\n",
        "                        \"choiceQuestion\": {\n",
        "                            \"type\": \"RADIO\",\n",
        "                            \"options\": [\n",
        "                                {\"value\": \"Model A reasoning is much better\"},\n",
        "                                {\"value\": \"Model A reasoning is slightly better\"},\n",
        "                                {\"value\": \"Both reasoning are equivalent\"},\n",
        "                                {\"value\": \"Model B reasoning is slightly better\"},\n",
        "                                {\"value\": \"Model B reasoning is much better\"}\n",
        "                            ]\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            \"location\": {\"index\": current_index}\n",
        "        }\n",
        "    })\n",
        "    current_index += 1\n",
        "    \n",
        "    # Expert score (optional)\n",
        "    all_requests.append({\n",
        "        \"createItem\": {\n",
        "            \"item\": {\n",
        "                \"title\": f\"Q5. What risk score would you assign (1-5)? - Sample {sample_num}  (Optional)\",\n",
        "                \"description\": \"\",\n",
        "                \"questionItem\": {\n",
        "                    \"question\": {\n",
        "                        \"required\": False,\n",
        "                        \"choiceQuestion\": {\n",
        "                            \"type\": \"RADIO\",\n",
        "                            \"options\": [\n",
        "                                {\"value\": \"1 - Very low risk\"},\n",
        "                                {\"value\": \"2 - Low risk\"},\n",
        "                                {\"value\": \"3 - Moderate risk\"},\n",
        "                                {\"value\": \"4 - High risk\"},\n",
        "                                {\"value\": \"5 - Very high risk\"}\n",
        "                            ]\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            \"location\": {\"index\": current_index}\n",
        "        }\n",
        "    })\n",
        "    current_index += 1\n",
        "    \n",
        "    # Comments\n",
        "    all_requests.append({\n",
        "        \"createItem\": {\n",
        "            \"item\": {\n",
        "                \"title\": f\"Q6. Any critical factors missed by all assessments or any comments? - Sample {sample_num} (Optional)\",\n",
        "                \"description\": \"\",\n",
        "                \"questionItem\": {\n",
        "                    \"question\": {\n",
        "                        \"required\": False,\n",
        "                        \"textQuestion\": {\"paragraph\": True}\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            \"location\": {\"index\": current_index}\n",
        "        }\n",
        "    })\n",
        "    current_index += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "batch_size = 5  # Small batches to avoid API limits\n",
        "success_count = 0\n",
        "\n",
        "for i in range(0, len(all_requests), batch_size):\n",
        "    batch = all_requests[i:i+batch_size]\n",
        "    \n",
        "    try:\n",
        "        forms_service.forms().batchUpdate(\n",
        "            formId=form_id,\n",
        "            body={\"requests\": batch}\n",
        "        ).execute()\n",
        "        success_count += len(batch)\n",
        "        print(f\"✅ Added {len(batch)} items (total: {success_count}/{len(all_requests)})\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Batch error: {e}\")\n",
        "        # Try individually\n",
        "        for j, request in enumerate(batch):\n",
        "            try:\n",
        "                forms_service.forms().batchUpdate(\n",
        "                    formId=form_id,\n",
        "                    body={\"requests\": [request]}\n",
        "                ).execute()\n",
        "                success_count += 1\n",
        "                print(f\"✅ Added item individually ({success_count}/{len(all_requests)})\")\n",
        "            except Exception as eq:\n",
        "                print(f\"❌ Individual error {i+j+1}: {eq}\")\n",
        "\n",
        "print(f\"📊 Successfully added {success_count}/{len(all_requests)} items\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📋 Form Information and Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 💾 Save Form Information - Get correct URLs from API\n",
        "\n",
        "try:\n",
        "    # Get the form data to extract the correct URLs\n",
        "    form_data = forms_service.forms().get(formId=form_id).execute()\n",
        "    \n",
        "    # Extract the correct URLs\n",
        "    edit_url = f\"https://docs.google.com/forms/d/{form_id}/edit\"\n",
        "    \n",
        "    # The responderUri gives us the correct public URL\n",
        "    if 'responderUri' in form_data:\n",
        "        view_url = form_data['responderUri']\n",
        "    else:\n",
        "        # Fallback URL format\n",
        "        view_url = f\"https://docs.google.com/forms/d/{form_id}/viewform\"\n",
        "    \n",
        "    # Responses URL - this goes to the responses tab of the edit page\n",
        "    responses_url = f\"https://docs.google.com/forms/d/{form_id}/edit#responses\"\n",
        "    \n",
        "    \n",
        "except Exception as e:\n",
        "    # Use fallback URLs\n",
        "    edit_url = f\"https://docs.google.com/forms/d/{form_id}/edit\"\n",
        "    view_url = f\"https://docs.google.com/forms/d/{form_id}/viewform\"\n",
        "    responses_url = f\"https://docs.google.com/forms/d/{form_id}/edit#responses\"\n",
        "\n",
        "form_info = {\n",
        "    'form_id': form_id,\n",
        "    'edit_url': edit_url,\n",
        "    'view_url': view_url,\n",
        "    'responses_url': responses_url,\n",
        "    'total_samples': len(sample_data),\n",
        "    'json_file_used': JSON_FILE_PATH,\n",
        "    'project_id': PROJECT_ID,\n",
        "    'form_title': FORM_TITLE\n",
        "}\n",
        "\n",
        "# Save to file\n",
        "with open(f'/content/form_result_{BATCH_ID}.json', 'w') as f:\n",
        "    json.dump(form_info, f, indent=2)\n",
        "\n",
        "\n",
        "print(f\"✏️  Edit Form: {edit_url}\")\n",
        "print(f\"👀 View Form: {view_url}\")\n",
        "print(f\"📈 Responses: {responses_url}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
